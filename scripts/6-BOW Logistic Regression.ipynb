{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NF-BOW Logistic Regression.ipynb","provenance":[],"collapsed_sections":["9NEq3EYHaxrh","LSgpTJUKyQEO","zrBbCnRZ4Bcf","NYIT_HTB4GDv","tJg4Ingw4b2X","QB0hcSNA6qM8"],"machine_shape":"hm","authorship_tag":"ABX9TyMC1P9g7vD0h7cpkKdVeX3P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9NEq3EYHaxrh"},"source":["# Introduction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"v5qN5wYYa1nk"},"source":["This code is trying to build movie review sentiment classifier using bag-of-words features and Logistic Regression as baseline"]},{"cell_type":"markdown","metadata":{"id":"LSgpTJUKyQEO"},"source":["# Import"]},{"cell_type":"code","metadata":{"id":"czO4c-DnwBQ1"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt \n","from gensim.parsing.preprocessing import remove_stopwords\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.porter import PorterStemmer\n","import torch\n","from sklearn.model_selection import train_test_split\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from gensim.models import Word2Vec\n","import gensim\n","from gensim import corpora\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","%matplotlib inline\n","\n","from collections import Counter\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrBbCnRZ4Bcf"},"source":["# load data and preprocessing"]},{"cell_type":"code","metadata":{"id":"WubqljrCyYZj"},"source":["df = pd.read_csv(\"movie_review_RT50K.csv\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"7bHe4Vagy68U","executionInfo":{"status":"ok","timestamp":1621648741707,"user_tz":240,"elapsed":3084,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"c6aee45b-70a1-4131-eb63-ce58873be0e8"},"source":["df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['content']] \n","df.head(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>sentiment</th>\n","      <th>tokenized_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Compelling in fits and starts, actor-director ...</td>\n","      <td>0</td>\n","      <td>[compelling, in, fits, and, starts, actor, dir...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Quite simply one of the finest comic romances ...</td>\n","      <td>1</td>\n","      <td>[quite, simply, one, of, the, finest, comic, r...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A psychological thriller that dangles over the...</td>\n","      <td>0</td>\n","      <td>[psychological, thriller, that, dangles, over,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             content  ...                                     tokenized_text\n","0  Compelling in fits and starts, actor-director ...  ...  [compelling, in, fits, and, starts, actor, dir...\n","1  Quite simply one of the finest comic romances ...  ...  [quite, simply, one, of, the, finest, comic, r...\n","2  A psychological thriller that dangles over the...  ...  [psychological, thriller, that, dangles, over,...\n","\n","[3 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"2_FtXH-3zXt8","executionInfo":{"status":"ok","timestamp":1621649095482,"user_tz":240,"elapsed":204,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"9e02e9b9-17d7-4d5b-9585-09c93861f355"},"source":["# porter_stemmer = PorterStemmer()\n","# df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df['tokenized_text'] ]\n","df.head(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>sentiment</th>\n","      <th>tokenized_text</th>\n","      <th>stemmed_tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Compelling in fits and starts, actor-director ...</td>\n","      <td>0</td>\n","      <td>[compelling, in, fits, and, starts, actor, dir...</td>\n","      <td>[compel, in, fit, and, start, actor, director,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Quite simply one of the finest comic romances ...</td>\n","      <td>1</td>\n","      <td>[quite, simply, one, of, the, finest, comic, r...</td>\n","      <td>[quit, simpli, on, of, the, finest, comic, rom...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A psychological thriller that dangles over the...</td>\n","      <td>0</td>\n","      <td>[psychological, thriller, that, dangles, over,...</td>\n","      <td>[psycholog, thriller, that, dangl, over, the, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             content  ...                                     stemmed_tokens\n","0  Compelling in fits and starts, actor-director ...  ...  [compel, in, fit, and, start, actor, director,...\n","1  Quite simply one of the finest comic romances ...  ...  [quit, simpli, on, of, the, finest, comic, rom...\n","2  A psychological thriller that dangles over the...  ...  [psycholog, thriller, that, dangl, over, the, ...\n","\n","[3 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFvhjQ95ybCl","executionInfo":{"status":"ok","timestamp":1621649262471,"user_tz":240,"elapsed":499,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"c77aec15-b3a5-4c0b-c480-7e663564e527"},"source":["X_train, X_test, Y_train, Y_test = train_test_split(df[['content','tokenized_text','stemmed_tokens']], \n","                                                    df['sentiment'], \n","                                                    shuffle=True,\n","                                                    test_size=0.2, \n","                                                    random_state=2021)\n","\n","print(X_train.shape)\n","print(X_test.shape)\n","print(Y_train.shape)\n","print(Y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(40000, 3)\n","(10000, 3)\n","(40000,)\n","(10000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NjxSR8OF4eC2"},"source":["change targets to tensor"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoLqnSxx2s9e","executionInfo":{"status":"ok","timestamp":1621649725202,"user_tz":240,"elapsed":387,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"772e9d1d-65d1-4f02-f6a3-8d63a24b0682"},"source":["train_y = torch.tensor(Y_train.tolist())\n","test_y = torch.tensor(Y_test.tolist())\n","print(train_y.shape)\n","print(test_y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([40000])\n","torch.Size([10000])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yuNb_ocm77GL"},"source":["train_X=X_train.stemmed_tokens\n","test_X=X_test.stemmed_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYIT_HTB4GDv"},"source":["# create function to generate bag of words feature"]},{"cell_type":"code","metadata":{"id":"yoPdZgZX1BkA"},"source":["def make_dict(df, padding=True):\n","    if padding:\n","        review_dict = corpora.Dictionary([['pad']])\n","        review_dict.add_documents(df['stemmed_tokens'])\n","    else:\n","        review_dict = corpora.Dictionary(df['stemmed_tokens'])\n","    return review_dict\n","\n","review_dict = make_dict(df, padding=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kw9AToW81qDd","executionInfo":{"status":"ok","timestamp":1621652862031,"user_tz":240,"elapsed":627,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"dc72633a-4bbe-4258-8a15-64024e676f5f"},"source":["\n","VOCAB_SIZE = len(review_dict)#24832\n","NUM_LABELS = 2\n","\n","def make_bow_vector(review_dict, sentence):\n","    vec = torch.zeros(VOCAB_SIZE+1, dtype=torch.float64)\n","    for word in sentence:\n","        vec[review_dict.token2id[word]] += 1\n","    return vec.view(1, -1).float()\n","\n","temp=make_bow_vector(review_dict,['compel','in','fit','and','start','actor','director'])\n","print(temp.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 24833])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0I8qyPfF3aav"},"source":["the size of this bag of words feature is 24832, which is impractical to generate before hand, you have to do it on gpu as you train the model"]},{"cell_type":"markdown","metadata":{"id":"tJg4Ingw4b2X"},"source":["# define logistic regression classifier"]},{"cell_type":"code","metadata":{"id":"wNE9e4KK4wy3"},"source":["class LRBOW(nn.Module):  \n","\n","    def __init__(self, num_labels, vocab_size):\n","\n","        super(LRBOW, self).__init__()\n","\n","        self.linear = nn.Linear(vocab_size, num_labels)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, bow_vec): \n","\n","        x=self.linear(bow_vec)\n","        x=self.softmax(x)\n","\n","        return x \n","\n","\n","# tempc=make_bow_vector(review_dict,X_train.stemmed_tokens[0])\n","# tempc.shape\n","# model=LRBOW(NUM_LABELS,VOCAB_SIZE)\n","# model(tempc)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QB0hcSNA6qM8"},"source":["# training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrLWjv_V-ftI","executionInfo":{"status":"ok","timestamp":1621651778069,"user_tz":240,"elapsed":204,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"5c62a6d1-b43c-4b7f-fac4-0f2282aad117"},"source":["trainLen=len(train_X)\n","testLen=len(test_y)\n","print(trainLen,testLen)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["40000 10000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"In8di3v7AJU0"},"source":["device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dmOIwUkF4Wk","executionInfo":{"status":"ok","timestamp":1621653906486,"user_tz":240,"elapsed":215996,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"35341157-d989-40d8-f175-d91ea22ddc7a"},"source":["%%time\n","BATCH_SIZE=100\n","VOCAB_SIZE = len(review_dict)#24832\n","NUM_LABELS = 2\n","\n","model=LRBOW(NUM_LABELS,VOCAB_SIZE+1) #+1 for the interception\n","model.to(device)\n","\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","num_epochs=20\n","model.train()\n","\n","for epoch in range(num_epochs):\n","  print(\"Epoch\" + str(epoch + 1),'started')\n","  model.zero_grad()\n","  trainLoss=0;\n","  for i in tqdm(range(0,trainLen,BATCH_SIZE)):\n","      model.zero_grad()\n","  \n","      endIndex=min(i+BATCH_SIZE,trainLen)\n","      labels=train_y[i:endIndex].to(device) \n","\n","      tempX=[]\n","\n","      for j in range(i,min(i+BATCH_SIZE,trainLen)):\n","        v = make_bow_vector(review_dict,train_X.iloc[j])\n","        tempX.append(v)        \n","\n","      tempX=torch.cat(tempX,dim=-2).to(device)\n","      labels=train_y[i:endIndex].to(device)\n","\n","      output = model(tempX)\n","\n","\n","      loss = loss_function(output, labels)\n","      trainLoss = trainLoss + loss.item()\n","      loss.backward()\n","      optimizer.step()\n","\n","\n","  trainLoss=round(trainLoss / trainLen,5)\n","  print('loss',loss.item())\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  1%|▏         | 5/400 [00:00<00:09, 42.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch1 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 36.41it/s]\n","  1%|▏         | 5/400 [00:00<00:10, 39.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6891687512397766\n","Epoch2 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.20it/s]\n","  1%|          | 4/400 [00:00<00:10, 38.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6825233697891235\n","Epoch3 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:11<00:00, 35.67it/s]\n","  1%|          | 4/400 [00:00<00:11, 35.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6752989292144775\n","Epoch4 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.03it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 40.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6682677268981934\n","Epoch5 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 36.82it/s]\n","  1%|          | 4/400 [00:00<00:12, 32.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6617194414138794\n","Epoch6 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:11<00:00, 34.61it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 39.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6556941866874695\n","Epoch7 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.51it/s]\n","  1%|          | 4/400 [00:00<00:09, 39.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6501526832580566\n","Epoch8 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 36.98it/s]\n","  1%|          | 3/400 [00:00<00:16, 23.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6450387835502625\n","Epoch9 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 38.63it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 42.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6402994394302368\n","Epoch10 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.43it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 42.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6358882188796997\n","Epoch11 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:11<00:00, 35.33it/s]\n","  1%|          | 3/400 [00:00<00:15, 25.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6317654848098755\n","Epoch12 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.67it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 42.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6278976202011108\n","Epoch13 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.13it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 40.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6242559552192688\n","Epoch14 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:11<00:00, 35.34it/s]\n","  1%|          | 3/400 [00:00<00:14, 26.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6208153963088989\n","Epoch15 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:11<00:00, 35.94it/s]\n","  1%|          | 3/400 [00:00<00:15, 25.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6175544857978821\n","Epoch16 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 38.46it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 40.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6144548654556274\n","Epoch17 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.58it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 40.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6115000247955322\n","Epoch18 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 38.51it/s]\n","  1%|          | 4/400 [00:00<00:10, 37.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6086758375167847\n","Epoch19 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:10<00:00, 37.31it/s]\n","  1%|▏         | 5/400 [00:00<00:09, 41.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6059700846672058\n","Epoch20 started\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:09<00:00, 41.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["loss 0.6033716797828674\n","CPU times: user 3min 35s, sys: 2.26 s, total: 3min 37s\n","Wall time: 3min 35s\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDaLLSTjGKOe","executionInfo":{"status":"ok","timestamp":1621653975988,"user_tz":240,"elapsed":3001,"user":{"displayName":"Yifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGx-4KNhiyqJe722L4Ims6mY4BhmIr-h8R4yRY=s64","userId":"13988890221464000575"}},"outputId":"e6e4baae-0198-40f8-c661-26c2ccdf1d58"},"source":["BATCH_SIZE=100\n","y_pred = []\n","model.eval()\n","\n","with torch.no_grad():\n","     for i in tqdm(range(0,testLen,BATCH_SIZE)):\n","        endIndex=min(i+BATCH_SIZE,testLen)\n","\n","        tempX=[]\n","\n","        for j in range(i,min(i+BATCH_SIZE,testLen)):\n","          v = make_bow_vector(review_dict,test_X.iloc[j])\n","          tempX.append(v)        \n","\n","        tempX=torch.cat(tempX,dim=-2).to(device)\n","        output = model(tempX)\n","\n","        predicted_class=torch.argmax(output,dim=-1).cpu().tolist()        \n","        y_pred.extend(predicted_class)      \n","\n","print()\n","print(classification_report(Y_test.tolist(),y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 100/100 [00:02<00:00, 35.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.73      0.73      5028\n","           1       0.72      0.71      0.72      4972\n","\n","    accuracy                           0.72     10000\n","   macro avg       0.72      0.72      0.72     10000\n","weighted avg       0.72      0.72      0.72     10000\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}